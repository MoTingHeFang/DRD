{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import ngrams\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全局变量\n",
    "sourceAttributeList = []\n",
    "#token的字典，用来记录token出现的次数nt\n",
    "tokenDict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将属性名按照标点，分割成token\n",
    "def snake_case_split(line):\n",
    "    #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "    #r表示非转义的原始字符串\n",
    "    #由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r'\\t'，等价于 '\\\\t')匹配相应的特殊字符。\n",
    "    #[...]用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k'\n",
    "    line_split = re.split(r'[\\s_]',line.strip())\n",
    "    line_split = [line.strip() for line in line_split if len(line.strip())>0]\n",
    "    return line_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性按照大写字母，分割成token\n",
    "#\"firstName\"分成 ['First', 'Name']\n",
    "def camel_case_split(str):\n",
    "    if \"a\"<=str[0]<=\"z\" :\n",
    "        strTemp=str[0].upper()+str[1:]\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', strTemp)\n",
    "    else :\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性名先按照snakeCase拆分，再按照camelCase拆分\n",
    "def attributeToToken(str):\n",
    "    tokenList = []\n",
    "    for snakeToken in snake_case_split(str):\n",
    "        for camelToken in camel_case_split(snakeToken):\n",
    "            camelToken = camelToken.lower()\n",
    "            corpusTokenList = ngrams.segment2(camelToken) [1]\n",
    "            tokenList = tokenList + corpusTokenList\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'heart', 'will', 'go', 'on', 'and', 'on']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = attributeToToken(\"myheartwillgoonandon\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute:\n",
    "    def __init__(self, attributeName, originateFrom):\n",
    "        self.attributeName = attributeName\n",
    "        self.tokenList = attributeToToken(attributeName)\n",
    "        #属性的词嵌入表示\n",
    "        #加权平均\n",
    "        #权和N有关\n",
    "        #numOfAttribute得读完整个数据湖才知道\n",
    "        #self.wordEmbedding = tokenToVector(self.tokenList)\n",
    "        \n",
    "        #属性来自哪张表\n",
    "        self.originateFrom = originateFrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c13_0____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_70941cace7dd1c45____c12_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_e9efd5cda78af711____c4_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_ca85e8f9eef5b9d5____c5_1____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_dc9442ed0b52d69c____c13_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/PED_SK_DTL_SNF____c6_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c21_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c7_0____2.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_014c2259cce4ab13____c25_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c2_1____3.csv\n"
     ]
    }
   ],
   "source": [
    "#getTokenWeight函数用到numOfAttribute\n",
    "numOfAttribute = 0\n",
    "numOfTable = 0\n",
    "\n",
    "#将湖中所有属性读到attributeList中\n",
    "#for循环中的info代表文件名\n",
    "for info in os.listdir('/usr/dataLake/datasets/BenchmarkCsvfiles'): \n",
    "    #去掉.csv后缀\n",
    "    tableName = info[0:-4]\n",
    "    domain = os.path.abspath('/usr/dataLake/datasets/BenchmarkCsvfiles') #获取文件夹的路径\n",
    "    info = os.path.join(domain,info) #将路径与文件名结合起来就是每个文件的完整路径\n",
    "    print(info)\n",
    "    df = pd.read_csv(info)\n",
    "    for attributeName in list(df):\n",
    "        #Attribute类的构造函数会更新tokenDict\n",
    "        x=Attribute(attributeName, tableName)\n",
    "        numOfAttribute = numOfAttribute+1\n",
    "        #通过append函数将自定义类的对象放入list中\n",
    "        sourceAttributeList.append(x)\n",
    "        #print(sourceAttributeList[i])可证明是Attribute类的对象\n",
    "    \n",
    "    numOfTable = numOfTable + 1\n",
    "    if numOfTable == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改完毕\n",
    "for sourceAttribute in sourceAttributeList:\n",
    "    for token in sourceAttribute.tokenList:\n",
    "        if token in tokenDict:\n",
    "              tokenDict[token] = tokenDict[token] + 1\n",
    "        else:\n",
    "              tokenDict[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 2, 'auth': 2, 'description': 4, 'granted': 2, 'date': 9, 'lic': 2, 'no': 5, 'pub': 2, 'text': 2, 'reg': 7, 'service': 6, 'number': 2, 'type': 9, 'other': 2, 'details': 3, 'tao': 2, 'covered': 2, 'by': 2, 'area': 2, 'effective': 2, 'end': 2, 'trading': 2, 'name': 4, 'quality': 25, 'indicator': 25, 'indicateurs': 29, 'de': 29, 'qualit': 29, 'kind': 1, 'pmc': 1, 'he': 1, 'pm': 4, 'te': 4, 's': 3, 'grou': 1, 'colour': 1, 'consis': 1, 'pls': 1, 'mottle': 1, 'frq': 1, 'root': 2, 'freq': 1, 'sec': 1, 'grad': 1, 'rest': 1, 'ri': 1, 'soil': 1, 'group': 1, 'exp': 1, 'op': 1, 'registration': 1, 'status': 1, 'subsidies': 1, 'finish': 1, 'point': 2, 'received': 1, 'start': 1, 'via': 1, 'a': 1, 'lightings': 1, 'boardings': 1, 'cross': 1, 'street': 3, 'location': 2, 'on': 2, 'routes': 1, 'stop': 2, 'id': 2, 'aircraft': 6, 'make': 4, 'airport': 1, 'icao': 1, 'incident': 1, 'occ': 2, 'ocean': 1, 'operator': 2, 'organization': 5, 'country': 5}\n"
     ]
    }
   ],
   "source": [
    "print(tokenDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "fasttext.FastText.eprint = lambda x: None\n",
    "model = fasttext.load_model(\"/usr/dataLake/result/fil9.bin\")\n",
    "\n",
    "def getTokenWeight(token):\n",
    "    nt = tokenDict[token]\n",
    "    #默认以e为底\n",
    "    return math.log(numOfAttribute/nt)\n",
    "\n",
    "#归一化向量\n",
    "def normalize(x):\n",
    "    return x/np.linalg.norm(x)\n",
    "\n",
    "#将属性名的tokenList转换为词嵌入向量\n",
    "#获取token对应的词嵌入向量，归一化。加权平均后，再归一化一下。\n",
    "def tokenToVector(tokenList):\n",
    "    #获取词嵌入\n",
    "    wordEmbedding = model.get_word_vector(tokenList[0])\n",
    "    #函数返回值\n",
    "    unitVector = getTokenWeight(tokenList[0]) * normalize(wordEmbedding)\n",
    "    #加权平均的分母\n",
    "    totalWeight = getTokenWeight(tokenList[0])\n",
    "    for token in tokenList[1:]:\n",
    "        wordEmbedding = model.get_word_vector(token)\n",
    "        unitVector = unitVector + getTokenWeight(token) * normalize(wordEmbedding)\n",
    "        totalWeight = totalWeight+getTokenWeight(token)    \n",
    "    return normalize(unitVector/totalWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算attributeVector，并输出到文件中\n",
    "\n",
    "with open('attributeVector.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for sourceAttribute in sourceAttributeList:\n",
    "        sourceAttribute.wordEmbedding = tokenToVector(sourceAttribute.tokenList)\n",
    "        writer.writerow(sourceAttribute.wordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Indicator\n",
      "Indicateurs de qualit�\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.2\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.3\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.4\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.5\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.6\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.7\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.8\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(16,25):\n",
    "    print(sourceAttributeList[i].attributeName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用list存储列的类别信息\n",
    "labels=[]\n",
    "with open('/usr/dataLake/DRD/ball-k-means/labels.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        labels.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69, 71], [13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 34, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], [9, 66], [81, 82, 83, 84, 85, 86], [96, 97, 98, 99, 100], [58], [26, 33], [78, 106], [2, 56, 61, 70], [6, 62], [79], [11, 68], [91, 92], [10, 67], [32, 50], [76], [75], [3, 57], [73], [77, 90], [30], [14, 15, 35, 36], [80, 107], [7, 8, 63, 64, 94, 95], [25, 74], [89], [93], [27, 47, 48, 49], [0, 1, 53, 54], [28, 52], [51], [12, 72], [55], [101, 102, 103, 104, 105], [87], [4, 59], [29, 31], [5, 60], [88], [65]]\n"
     ]
    }
   ],
   "source": [
    "#已经检查通过 20221129   kdzkdz\n",
    "def read_csv(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    content = f.read()\n",
    "    final_list = list()\n",
    "    rows = content.split('\\n')\n",
    "    #-1是为了删除末尾的空列表\n",
    "    for row in rows[0:-1]:\n",
    "        str_list = row.split(',')\n",
    "        #将字符串转换为int\n",
    "        num_list = [int(i) for i in str_list]        \n",
    "        final_list.append(num_list)\n",
    "    return final_list\n",
    "\n",
    "#二维list，每个元素是int\n",
    "#数据正确性已经验证\n",
    "clusterPointIndex = read_csv(\"/usr/dataLake/DRD/ball-k-means/ClusterPointIndex.csv\")\n",
    "print(clusterPointIndex)\n",
    "#已经检查通过 20221129   kdzkdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=40\n",
    "#键 是 表名，值 为 列表\n",
    "CandidateTablePair={}\n",
    "\n",
    "#循环40个簇\n",
    "for cluster in range(0, 40 ):\n",
    "    for i in range(0 , len(clusterPointIndex[cluster])-1 ):\n",
    "        for j in range(i+1, len(clusterPointIndex[cluster]) ):\n",
    "            CandidateTablePair\n",
    "\n",
    "###todo：！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入target表\n",
    "#target表作为增量，放到程序的最后\n",
    "targetTable='t_93f3d6f7fc6aa6ff____c13_0____0'\n",
    "targetAttributeList = []\n",
    "info = targetTable+'.csv'\n",
    "domain = os.path.abspath('/usr/dataLake/datasets/BenchmarkCsvfiles') #获取文件夹的路径\n",
    "info = os.path.join(domain,info) #将路径与文件名结合起来就是每个文件的完整路径\n",
    "df = pd.read_csv(info)\n",
    "for targetAttributeName in list(df):\n",
    "    #targetAttribute的token被额外加了一次\n",
    "    targetAttribute=Attribute(targetAttributeName, targetTable)\n",
    "    targetAttributeList.append(targetAttribute)\n",
    "\n",
    "### todo：有新表应该更新tokenDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
