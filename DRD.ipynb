{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import ngrams\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全局变量\n",
    "attributeList = []\n",
    "#token的字典，用来记录token出现的次数nt\n",
    "tokenDict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将属性名按照标点，分割成token\n",
    "def snake_case_split(line):\n",
    "    #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "    #r表示非转义的原始字符串\n",
    "    #由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r'\\t'，等价于 '\\\\t')匹配相应的特殊字符。\n",
    "    #[...]用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k'\n",
    "    line_split = re.split(r'[\\s_]',line.strip())\n",
    "    line_split = [line.strip() for line in line_split if len(line.strip())>0]\n",
    "    return line_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性按照大写字母，分割成token\n",
    "#\"firstName\"分成 ['First', 'Name']\n",
    "def camel_case_split(str):\n",
    "    if \"a\"<=str[0]<=\"z\" :\n",
    "        strTemp=str[0].upper()+str[1:]\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', strTemp)\n",
    "    else :\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性名先按照snakeCase拆分，再按照camelCase拆分\n",
    "def attributeToToken(str):\n",
    "    tokenList = []\n",
    "    for snakeToken in snake_case_split(str):\n",
    "        for camelToken in camel_case_split(snakeToken):\n",
    "            camelToken = camelToken.lower()\n",
    "            corpusTokenList = ngrams.segment2(camelToken) [1]\n",
    "            tokenList = tokenList + corpusTokenList\n",
    "            for finalToken in corpusTokenList:                \n",
    "                #生成token的时候就更新tokenDict\n",
    "                if finalToken in tokenDict:\n",
    "                    tokenDict[finalToken] = tokenDict[finalToken]+1\n",
    "                else:\n",
    "                    tokenDict[finalToken] = 1\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute:\n",
    "    def __init__(self, attributeName, originateFrom):\n",
    "        self.attributeName = attributeName\n",
    "        self.tokenList = attributeToToken(attributeName)\n",
    "        #属性的词嵌入表示\n",
    "        #加权平均\n",
    "        #权和N有关\n",
    "        #numOfAttribute得读完整个数据湖才知道\n",
    "        #self.wordEmbedding = tokenToVector(self.tokenList)\n",
    "        \n",
    "        #属性来自哪张表\n",
    "        self.originateFrom = originateFrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c13_0____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_70941cace7dd1c45____c12_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_e9efd5cda78af711____c4_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_ca85e8f9eef5b9d5____c5_1____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_dc9442ed0b52d69c____c13_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/PED_SK_DTL_SNF____c6_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c21_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c7_0____2.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_014c2259cce4ab13____c25_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c2_1____3.csv\n"
     ]
    }
   ],
   "source": [
    "#getTokenWeight函数用到numOfAttribute\n",
    "numOfAttribute = 0\n",
    "numOfTable = 0\n",
    "\n",
    "#将湖中所有属性读到attributeList中\n",
    "#for循环中的info代表文件名\n",
    "for info in os.listdir('/usr/dataLake/datasets/BenchmarkCsvfiles'): \n",
    "    #去掉.csv后缀\n",
    "    tableName = info[0:-4]\n",
    "    domain = os.path.abspath('/usr/dataLake/datasets/BenchmarkCsvfiles') #获取文件夹的路径\n",
    "    info = os.path.join(domain,info) #将路径与文件名结合起来就是每个文件的完整路径\n",
    "    print(info)\n",
    "    df = pd.read_csv(info)\n",
    "    for attributeName in list(df):\n",
    "        #Attribute类的构造函数会更新tokenDict\n",
    "        x=Attribute(attributeName, tableName)\n",
    "        numOfAttribute = numOfAttribute+1\n",
    "        #通过append函数将自定义类的对象放入list中\n",
    "        attributeList.append(x)\n",
    "        #print(attributeList[i])可证明是Attribute类的对象\n",
    "    \n",
    "    numOfTable = numOfTable + 1\n",
    "    if numOfTable == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 2, 'auth': 2, 'description': 4, 'granted': 2, 'date': 9, 'lic': 2, 'no': 5, 'pub': 2, 'text': 2, 'reg': 7, 'service': 6, 'number': 2, 'type': 9, 'other': 2, 'details': 3, 'tao': 2, 'covered': 2, 'by': 2, 'area': 2, 'effective': 2, 'end': 2, 'trading': 2, 'name': 4, 'quality': 25, 'indicator': 25, 'indicateurs': 29, 'de': 29, 'qualit': 29, 'kind': 1, 'pmc': 1, 'he': 1, 'pm': 4, 'te': 4, 's': 3, 'grou': 1, 'colour': 1, 'consis': 1, 'pls': 1, 'mottle': 1, 'frq': 1, 'root': 2, 'freq': 1, 'sec': 1, 'grad': 1, 'rest': 1, 'ri': 1, 'soil': 1, 'group': 1, 'exp': 1, 'op': 1, 'registration': 1, 'status': 1, 'subsidies': 1, 'finish': 1, 'point': 2, 'received': 1, 'start': 1, 'via': 1, 'a': 1, 'lightings': 1, 'boardings': 1, 'cross': 1, 'street': 3, 'location': 2, 'on': 2, 'routes': 1, 'stop': 2, 'id': 2, 'aircraft': 6, 'make': 4, 'airport': 1, 'icao': 1, 'incident': 1, 'occ': 2, 'ocean': 1, 'operator': 2, 'organization': 5, 'country': 5}\n"
     ]
    }
   ],
   "source": [
    "print(tokenDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入target表\n",
    "targetTable='t_93f3d6f7fc6aa6ff____c13_0____0'\n",
    "targetAttributeList = []\n",
    "info = targetTable+'.csv'\n",
    "domain = os.path.abspath('/usr/dataLake/datasets/BenchmarkCsvfiles') #获取文件夹的路径\n",
    "info = os.path.join(domain,info) #将路径与文件名结合起来就是每个文件的完整路径\n",
    "df = pd.read_csv(info)\n",
    "for targetAttributeName in list(df):\n",
    "    #targetAttribute的token被额外加了一次\n",
    "    targetAttribute=Attribute(targetAttributeName, targetTable)\n",
    "    targetAttributeList.append(targetAttribute)\n",
    "    for token in targetAttribute.tokenList:\n",
    "        #矫正tokendict\n",
    "        #已经通过验证\n",
    "        tokenDict[token] = tokenDict[token]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 2, 'auth': 2, 'description': 4, 'granted': 2, 'date': 9, 'lic': 2, 'no': 5, 'pub': 2, 'text': 2, 'reg': 7, 'service': 6, 'number': 2, 'type': 9, 'other': 2, 'details': 3, 'tao': 2, 'covered': 2, 'by': 2, 'area': 2, 'effective': 2, 'end': 2, 'trading': 2, 'name': 4, 'quality': 25, 'indicator': 25, 'indicateurs': 29, 'de': 29, 'qualit': 29, 'kind': 1, 'pmc': 1, 'he': 1, 'pm': 4, 'te': 4, 's': 3, 'grou': 1, 'colour': 1, 'consis': 1, 'pls': 1, 'mottle': 1, 'frq': 1, 'root': 2, 'freq': 1, 'sec': 1, 'grad': 1, 'rest': 1, 'ri': 1, 'soil': 1, 'group': 1, 'exp': 1, 'op': 1, 'registration': 1, 'status': 1, 'subsidies': 1, 'finish': 1, 'point': 2, 'received': 1, 'start': 1, 'via': 1, 'a': 1, 'lightings': 1, 'boardings': 1, 'cross': 1, 'street': 3, 'location': 2, 'on': 2, 'routes': 1, 'stop': 2, 'id': 2, 'aircraft': 6, 'make': 4, 'airport': 1, 'icao': 1, 'incident': 1, 'occ': 2, 'ocean': 1, 'operator': 2, 'organization': 5, 'country': 5}\n"
     ]
    }
   ],
   "source": [
    "print(tokenDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "\n",
    "fasttext.FastText.eprint = lambda x: None\n",
    "model = fasttext.load_model(\"/usr/dataLake/result/fil9.bin\")\n",
    "\n",
    "def getTokenWeight(token):\n",
    "    nt = tokenDict[token]\n",
    "    #默认以e为底\n",
    "    return math.log(numOfAttribute/nt)\n",
    "\n",
    "#归一化向量\n",
    "def normalize(x):\n",
    "    return x/np.linalg.norm(x)\n",
    "\n",
    "#将属性名的tokenList转换为词嵌入向量\n",
    "#获取token对应的词嵌入向量，归一化。加权平均后，再归一化一下。\n",
    "def tokenToVector(tokenList):\n",
    "    #获取词嵌入\n",
    "    wordEmbedding = model.get_word_vector(tokenList[0])\n",
    "    #函数返回值\n",
    "    unitVector = getTokenWeight(tokenList[0]) * normalize(wordEmbedding)\n",
    "    #加权平均的分母\n",
    "    totalWeight = getTokenWeight(tokenList[0])\n",
    "    for token in tokenList[1:]:\n",
    "        wordEmbedding = model.get_word_vector(token)\n",
    "        unitVector = unitVector + getTokenWeight(token) * normalize(wordEmbedding)\n",
    "        totalWeight = totalWeight+getTokenWeight(token)    \n",
    "    return normalize(unitVector/totalWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算attributeVector，并输出到文件中\n",
    "with open('attributeVector.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for targetAttribute in targetAttributeList:\n",
    "        targetAttribute.wordEmbedding = tokenToVector(targetAttribute.tokenList)\n",
    "        writer.writerow(targetAttribute.wordEmbedding)\n",
    "\n",
    "    for attribute in attributeList:\n",
    "        attribute.wordEmbedding = tokenToVector(attribute.tokenList)\n",
    "        writer.writerow(attribute.wordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Indicator\n",
      " Indicateurs de qualit�\n",
      "Quality Indicator\n",
      " Indicateurs de qualit� Indicateurs de qualit�\n",
      "Quality Indicator\n",
      " Indicateurs de qualit� Indicateurs de qualit�.1\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.2\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.3\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.4\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.5\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.6\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.7\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.8\n",
      "Quality Indicator\n",
      "Indicateurs de qualit�.9\n"
     ]
    }
   ],
   "source": [
    "for i in range(13,25):\n",
    "    print(attributeList[i].attributeName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用list存储列的类别信息\n",
    "ClusteringResults=[]\n",
    "with open('/usr/dataLake/DRD/ball-k-means/ClusteringResults.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        ClusteringResults.append(int(line))\n",
    "\n",
    "#已经验证ClusteringResults的长度\n",
    "#元素的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=40\n",
    "CandidateTablePair={}\n",
    "#直接把数据读到Clust中，等价转换\n",
    "ClusterPointIndex=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['94', '95', '96', '97'], ['105'], ['4', '17', '72'], ['27', '28', '48', '49'], ['8', '21', '77'], ['90', '103'], ['114', '115', '116', '117', '118'], ['3', '16', '70'], ['63'], ['10', '23', '80'], ['109', '110', '111', '112', '113'], ['41', '71'], ['64'], ['42', '44'], ['5', '18', '73'], ['40', '60', '61', '62'], ['0', '13', '66'], ['88', '91', '119'], ['45'], ['9', '22', '79'], ['89'], ['1', '14', '67'], ['39', '65'], ['26', '29', '30', '31', '32', '33', '34', '35', '36', '37', '47', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59'], ['92'], ['86'], ['98', '99', '107', '108'], ['93', '120'], ['2', '15', '69', '74', '83'], ['100', '101'], ['104'], ['12', '25', '85'], ['78'], ['46', '68'], ['11', '24', '81'], ['6', '19', '75'], ['106'], ['38'], ['7', '20', '43', '76', '87', '102'], ['82', '84']]\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    content = f.read()\n",
    "    final_list = list()\n",
    "    rows = content.split('\\n')\n",
    "    for row in rows:\n",
    "        final_list.append(row.split(','))\n",
    "    return final_list\n",
    "\n",
    "#二维list\n",
    "#type已经验证\n",
    "#二维list数据正确性已经验证\n",
    "#-1是为了删除末尾的空列表\n",
    "ClusterPointIndex = read_csv(\"/usr/dataLake/DRD/ball-k-means/ClusterPointIndex.csv\")[0:-1]\n",
    "print(ClusterPointIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nt小，ITF大，权重大。\n",
    "#token越稀有，权重越大。\n",
    "#N是数据湖中有多少列。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
