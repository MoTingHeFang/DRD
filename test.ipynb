{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import ngrams\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全局变量\n",
    "attributeList = []\n",
    "#token的字典，用来记录token出现的次数nt\n",
    "tokenDict={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将属性名按照标点，分割成token\n",
    "def snake_case_split(line):\n",
    "    #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "    #r表示非转义的原始字符串\n",
    "    #由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r'\\t'，等价于 '\\\\t')匹配相应的特殊字符。\n",
    "    #[...]用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k'\n",
    "    line_split = re.split(r'[\\s_]',line.strip())\n",
    "    line_split = [line.strip() for line in line_split if len(line.strip())>0]\n",
    "    return line_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性按照大写字母，分割成token\n",
    "#\"firstName\"分成 ['First', 'Name']\n",
    "def camel_case_split(str):\n",
    "    if \"a\"<=str[0]<=\"z\" :\n",
    "        strTemp=str[0].upper()+str[1:]\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', strTemp)\n",
    "    else :\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性名先按照snakeCase拆分，再按照camelCase拆分\n",
    "def attributeToToken(str):\n",
    "    tokenList = []\n",
    "    for snakeToken in snake_case_split(str):\n",
    "        for camelToken in camel_case_split(snakeToken):\n",
    "            camelToken = camelToken.lower()\n",
    "            corpusTokenList = ngrams.segment2(camelToken) [1]\n",
    "            tokenList = tokenList + corpusTokenList\n",
    "            for finalToken in corpusTokenList:                \n",
    "                #生成token的时候就更新tokenDict\n",
    "                if finalToken in tokenDict:\n",
    "                    tokenDict[finalToken] = tokenDict[finalToken]+1\n",
    "                else:\n",
    "                    tokenDict[finalToken] = 1       \n",
    "    return tokenList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute:\n",
    "    def __init__(self, attributeName, originateFrom):\n",
    "        self.attributeName = attributeName\n",
    "        self.tokenList = attributeToToken(attributeName)\n",
    "        #属性的词嵌入表示\n",
    "        #加权平均\n",
    "        #权和N有关\n",
    "        #numOfAttribute得读完整个数据湖才知道\n",
    "        #self.wordEmbedding = tokenToVector(self.tokenList)\n",
    "        \n",
    "        #属性来自哪张表\n",
    "        self.originateFrom = originateFrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alightings', 'boardings', 'cross_street', 'location', 'routes']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c5_0____1.csv')\n",
    "\n",
    "#list函数返回一个表的所有属性，返回一个list\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c13_0____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_70941cace7dd1c45____c12_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_e9efd5cda78af711____c4_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_ca85e8f9eef5b9d5____c5_1____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_dc9442ed0b52d69c____c13_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/PED_SK_DTL_SNF____c6_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c21_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c7_0____2.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_014c2259cce4ab13____c25_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c2_1____3.csv\n"
     ]
    }
   ],
   "source": [
    "numOfAttribute = 0\n",
    "numOfTable = 0\n",
    "\n",
    "#将湖中所有属性读到attributeList中\n",
    "\n",
    "#for循环中的info代表文件名\n",
    "for info in os.listdir('/usr/dataLake/datasets/BenchmarkCsvfiles'): \n",
    "    #去掉.csv后缀\n",
    "    tableName = info[0:-4]\n",
    "    domain = os.path.abspath('/usr/dataLake/datasets/BenchmarkCsvfiles') #获取文件夹的路径\n",
    "    info = os.path.join(domain,info) #将路径与文件名结合起来就是每个文件的完整路径\n",
    "    print(info)\n",
    "    df = pd.read_csv(info)\n",
    "    for attributeName in list(df):\n",
    "        x=Attribute(attributeName, tableName)\n",
    "        numOfAttribute = numOfAttribute+1\n",
    "        #通过append函数将自定义类的对象放入list中\n",
    "        attributeList.append(x)\n",
    "        #print(attributeList[i])可证明是Attribute类的对象\n",
    "    \n",
    "    numOfTable = numOfTable + 1\n",
    "    if numOfTable == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "import fasttext\n",
    "fasttext.FastText.eprint = lambda x: None\n",
    "model = fasttext.load_model(\"/usr/dataLake/result/fil9.bin\")\n",
    "\n",
    "def getTokenWeight(token):\n",
    "    nt = tokenDict[token]\n",
    "    #默认以e为底\n",
    "    return math.log(numOfAttribute/nt)\n",
    "\n",
    "#归一化向量\n",
    "def normalize(x):\n",
    "    return x/np.linalg.norm(x)\n",
    "\n",
    "#将属性名的tokenList转换为词嵌入向量\n",
    "def tokenToVector(tokenList):\n",
    "    #获取词嵌入\n",
    "    wordEmbedding = model.get_word_vector(tokenList[0])\n",
    "    #函数返回值\n",
    "    unitVector = getTokenWeight(tokenList[0]) * normalize(wordEmbedding)\n",
    "    #加权平均的分母\n",
    "    totalWeight = getTokenWeight(tokenList[0])\n",
    "    for token in tokenList[1:]:\n",
    "        wordEmbedding = model.get_word_vector(token)\n",
    "        unitVector = unitVector + getTokenWeight(token) * normalize(wordEmbedding)\n",
    "        totalWeight = totalWeight+getTokenWeight(token)    \n",
    "    return normalize(unitVector/totalWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.03972\n",
      "3.3230894\n"
     ]
    }
   ],
   "source": [
    "x=model.get_word_vector(\"address\")\n",
    "y=model.get_word_vector(\"location\")\n",
    "\n",
    "#x = np.array([3,4])\n",
    "print(np.linalg.norm(x))\n",
    "print(np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in attributeList:\n",
    "    attribute.wordEmbedding = tokenToVector(attribute.tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 2, 'auth': 2, 'description': 4, 'granted': 2, 'date': 9, 'lic': 2, 'no': 5, 'pub': 2, 'text': 2, 'reg': 7, 'service': 6, 'number': 2, 'type': 9, 'other': 2, 'details': 3, 'tao': 2, 'covered': 2, 'by': 2, 'area': 2, 'effective': 2, 'end': 2, 'trading': 2, 'name': 4, 'quality': 25, 'indicator': 25, 'indicateurs': 29, 'de': 29, 'qualit': 29, 'kind': 1, 'pmc': 1, 'he': 1, 'pm': 4, 'te': 4, 's': 3, 'grou': 1, 'colour': 1, 'consis': 1, 'pls': 1, 'mottle': 1, 'frq': 1, 'root': 2, 'freq': 1, 'sec': 1, 'grad': 1, 'rest': 1, 'ri': 1, 'soil': 1, 'group': 1, 'exp': 1, 'op': 1, 'registration': 1, 'status': 1, 'subsidies': 1, 'finish': 1, 'point': 2, 'received': 1, 'start': 1, 'via': 1, 'a': 1, 'lightings': 1, 'boardings': 1, 'cross': 1, 'street': 3, 'location': 2, 'on': 2, 'routes': 1, 'stop': 2, 'id': 2, 'aircraft': 6, 'make': 4, 'airport': 1, 'icao': 1, 'incident': 1, 'occ': 2, 'ocean': 1, 'operator': 2, 'organization': 5, 'country': 5}\n"
     ]
    }
   ],
   "source": [
    "print(tokenDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service', 'type', 'other', 'details']\n",
      "[ 4.99052927e-02 -1.15369272e-03  2.00742289e-01 -4.38491106e-02\n",
      "  1.17201284e-01  8.15718919e-02 -8.00280049e-02  1.59513593e-01\n",
      " -2.02500567e-01 -1.02051534e-01  7.91407302e-02  2.52465960e-02\n",
      " -1.18822321e-01  1.35037024e-02 -5.05634993e-02 -1.66779220e-01\n",
      "  8.83953422e-02  1.41523227e-01  4.71826224e-03 -7.99975023e-02\n",
      "  1.20108239e-01  6.35233298e-02  1.26938149e-01  6.46374896e-02\n",
      " -8.48486796e-02 -1.54228965e-02 -3.02484389e-02  9.51124821e-03\n",
      " -3.88216376e-02  6.81806058e-02  5.53127006e-03  1.19844461e-02\n",
      "  1.85125560e-01 -2.16972586e-02 -1.89874336e-01  7.05159530e-02\n",
      "  5.34744784e-02 -1.48780737e-02  2.37860009e-02  8.09602663e-02\n",
      "  2.32513860e-01  5.54126427e-02  2.85403877e-02 -2.36775354e-02\n",
      "  1.89561043e-02 -1.91506684e-01  3.88298668e-02  5.75303771e-02\n",
      "  2.17782855e-01 -7.10429624e-02 -1.60020217e-02 -1.24431234e-02\n",
      " -1.19365724e-02 -1.16756596e-01  8.47585872e-02 -1.34473620e-02\n",
      " -9.02329162e-02 -5.49329538e-03 -1.54544473e-01  1.07101090e-01\n",
      "  7.43038803e-02  1.47523686e-01 -7.43026957e-02  1.80050079e-02\n",
      " -5.35282353e-03  6.64382875e-02 -2.01695897e-02  4.50363085e-02\n",
      " -1.23904757e-01  7.41171688e-02 -8.15934539e-02 -1.49648368e-01\n",
      "  2.77011073e-04 -7.39795938e-02  1.64494999e-02  1.47680506e-01\n",
      "  1.24293774e-01  2.69822087e-02  1.07785650e-01  1.40569448e-01\n",
      "  6.49997219e-02 -5.05994000e-02  7.76663050e-02  1.64126277e-01\n",
      "  1.27566636e-01 -6.28029928e-02  1.65333226e-02 -8.91319886e-02\n",
      " -1.69131875e-01 -7.58371726e-02 -1.96857806e-02 -1.09947948e-02\n",
      "  4.85157482e-02 -9.42567438e-02 -8.97485167e-02 -2.95085222e-01\n",
      "  4.93505038e-02  1.00619175e-01  1.25448614e-01  7.51630366e-02]\n",
      "1.0\n",
      "t_93f3d6f7fc6aa6ff____c13_0____0\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "print(attributeList[8].tokenList)\n",
    "print(attributeList[8].wordEmbedding)\n",
    "print(np.linalg.norm(attributeList[8].wordEmbedding))\n",
    "print(attributeList[8].originateFrom)\n",
    "print(numOfAttribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.99052927e-02 -1.15369272e-03  2.00742289e-01 -4.38491106e-02\n",
      "  1.17201284e-01  8.15718919e-02 -8.00280049e-02  1.59513593e-01\n",
      " -2.02500567e-01 -1.02051534e-01  7.91407302e-02  2.52465960e-02\n",
      " -1.18822321e-01  1.35037024e-02 -5.05634993e-02 -1.66779220e-01\n",
      "  8.83953422e-02  1.41523227e-01  4.71826224e-03 -7.99975023e-02\n",
      "  1.20108239e-01  6.35233298e-02  1.26938149e-01  6.46374896e-02\n",
      " -8.48486796e-02 -1.54228965e-02 -3.02484389e-02  9.51124821e-03\n",
      " -3.88216376e-02  6.81806058e-02  5.53127006e-03  1.19844461e-02\n",
      "  1.85125560e-01 -2.16972586e-02 -1.89874336e-01  7.05159530e-02\n",
      "  5.34744784e-02 -1.48780737e-02  2.37860009e-02  8.09602663e-02\n",
      "  2.32513860e-01  5.54126427e-02  2.85403877e-02 -2.36775354e-02\n",
      "  1.89561043e-02 -1.91506684e-01  3.88298668e-02  5.75303771e-02\n",
      "  2.17782855e-01 -7.10429624e-02 -1.60020217e-02 -1.24431234e-02\n",
      " -1.19365724e-02 -1.16756596e-01  8.47585872e-02 -1.34473620e-02\n",
      " -9.02329162e-02 -5.49329538e-03 -1.54544473e-01  1.07101090e-01\n",
      "  7.43038803e-02  1.47523686e-01 -7.43026957e-02  1.80050079e-02\n",
      " -5.35282353e-03  6.64382875e-02 -2.01695897e-02  4.50363085e-02\n",
      " -1.23904757e-01  7.41171688e-02 -8.15934539e-02 -1.49648368e-01\n",
      "  2.77011073e-04 -7.39795938e-02  1.64494999e-02  1.47680506e-01\n",
      "  1.24293774e-01  2.69822087e-02  1.07785650e-01  1.40569448e-01\n",
      "  6.49997219e-02 -5.05994000e-02  7.76663050e-02  1.64126277e-01\n",
      "  1.27566636e-01 -6.28029928e-02  1.65333226e-02 -8.91319886e-02\n",
      " -1.69131875e-01 -7.58371726e-02 -1.96857806e-02 -1.09947948e-02\n",
      "  4.85157482e-02 -9.42567438e-02 -8.97485167e-02 -2.95085222e-01\n",
      "  4.93505038e-02  1.00619175e-01  1.25448614e-01  7.51630366e-02]\n"
     ]
    }
   ],
   "source": [
    "fenzi=math.log(108/6)*normalize(model.get_word_vector(\"service\"))+math.log(108/9)*normalize(model.get_word_vector(\"type\"))+math.log(108/2)*normalize(model.get_word_vector(\"other\"))+math.log(108/3)*normalize(model.get_word_vector(\"details\"))\n",
    "fenmu=math.log(108/6)+math.log(108/9)+math.log(108/2)+math.log(108/3)\n",
    "print(normalize(fenzi/fenmu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nt小，ITF大，权重大。\n",
    "#token越稀有，权重越大。\n",
    "#N是数据湖中有多少列。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
