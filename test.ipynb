{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import ngrams\n",
    "\n",
    "\n",
    "attributeList = []\n",
    "#token的字典，用来记录token出现的次数nt\n",
    "tokenDict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将属性名按照标点，分割成token\n",
    "def snake_case_split(line):\n",
    "    #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "    #r表示非转义的原始字符串\n",
    "    #由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r'\\t'，等价于 '\\\\t')匹配相应的特殊字符。\n",
    "    #[...]用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k'\n",
    "    line_split = re.split(r'[\\s_]',line.strip())\n",
    "    line_split = [line.strip() for line in line_split if len(line.strip())>0]\n",
    "    return line_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性按照大写字母，分割成token\n",
    "#\"firstName\"分成 ['First', 'Name']\n",
    "def camel_case_split(str):\n",
    "    if \"a\"<=str[0]<=\"z\" :\n",
    "        strTemp=str[0].upper()+str[1:]\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', strTemp)\n",
    "    else :\n",
    "        return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将属性名先按照snakeCase拆分，再按照camelCase拆分\n",
    "def attributeToToken(str):\n",
    "    tokenList = []\n",
    "    for snakeToken in snake_case_split(str):\n",
    "        for camelToken in camel_case_split(snakeToken):\n",
    "            camelToken = camelToken.lower()\n",
    "            corpusTokenList = ngrams.segment2(camelToken) [1]\n",
    "            tokenList = tokenList + corpusTokenList\n",
    "            for finalToken in corpusTokenList:                \n",
    "                #生成token的时候就更新tokenDict\n",
    "                if finalToken in tokenDict:\n",
    "                    tokenDict[finalToken] = tokenDict[finalToken]+1\n",
    "                else:\n",
    "                    tokenDict[finalToken] = 1       \n",
    "    return tokenList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "import fasttext\n",
    "fasttext.FastText.eprint = lambda x: None\n",
    "model = fasttext.load_model(\"/usr/dataLake/result/fil9.bin\")\n",
    "\n",
    "\n",
    "#将属性名的tokenList转换为词嵌入向量\n",
    "def tokenToVector(tokenList):\n",
    "    wordEmbedding=model.get_word_vector(tokenList[0])\n",
    "    length= len(tokenList)\n",
    "    for token in tokenList[1:]:\n",
    "        wordEmbedding=wordEmbedding+model.get_word_vector(token)\n",
    "    return wordEmbedding/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute:\n",
    "    def __init__(self, attributeName, originateFrom):\n",
    "        self.attributeName = attributeName\n",
    "        self.tokenList = attributeToToken(attributeName)\n",
    "        #属性的词嵌入表示\n",
    "        self.wordEmbedding = tokenToVector(self.tokenList)\n",
    "        #属性来自哪张表\n",
    "        self.originateFrom = originateFrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alightings', 'boardings', 'cross_street', 'location', 'routes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c5_0____1.csv')\n",
    "\n",
    "#list函数返回一个表的所有属性，返回一个list\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c13_0____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_70941cace7dd1c45____c12_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_e9efd5cda78af711____c4_1____1.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_ca85e8f9eef5b9d5____c5_1____0.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_dc9442ed0b52d69c____c13_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/PED_SK_DTL_SNF____c6_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_93f3d6f7fc6aa6ff____c21_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c7_0____2.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/t_014c2259cce4ab13____c25_0____4.csv\n",
      "/usr/dataLake/datasets/BenchmarkCsvfiles/CTA_Ridership_Avg_Weekday_Bus_Stop_Boardings_in_October_2012____c2_1____3.csv\n"
     ]
    }
   ],
   "source": [
    "numOfAttribute = 0\n",
    "#for循环中的info代表文件名\n",
    "\n",
    "numoftable = 0\n",
    "\n",
    "for info in os.listdir('/usr/dataLake/datasets/BenchmarkCsvfiles'): \n",
    "    #去掉.csv后缀\n",
    "    tableName = info[0:-4]\n",
    "    domain = os.path.abspath('/usr/dataLake/datasets/BenchmarkCsvfiles') #获取文件夹的路径\n",
    "    info = os.path.join(domain,info) #将路径与文件名结合起来就是每个文件的完整路径\n",
    "    print(info)\n",
    "    df = pd.read_csv(info)\n",
    "    for attributeName in list(df):\n",
    "        x=Attribute(attributeName, tableName)\n",
    "        numOfAttribute = numOfAttribute+1\n",
    "        #通过append函数将自定义类的对象放入list中\n",
    "        attributeList.append(x)\n",
    "        #print(attributeList[i])可证明是Attribute类的对象\n",
    "    \n",
    "    numoftable= numoftable+1\n",
    "    if numoftable == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 2, 'auth': 2, 'description': 4, 'granted': 2, 'date': 9, 'lic': 2, 'no': 5, 'pub': 2, 'text': 2, 'reg': 7, 'service': 6, 'number': 2, 'type': 9, 'other': 2, 'details': 3, 'tao': 2, 'covered': 2, 'by': 2, 'area': 2, 'effective': 2, 'end': 2, 'trading': 2, 'name': 4, 'quality': 25, 'indicator': 25, 'indicateurs': 29, 'de': 29, 'qualit': 29, 'kind': 1, 'pmc': 1, 'he': 1, 'pm': 4, 'te': 4, 's': 3, 'grou': 1, 'colour': 1, 'consis': 1, 'pls': 1, 'mottle': 1, 'frq': 1, 'root': 2, 'freq': 1, 'sec': 1, 'grad': 1, 'rest': 1, 'ri': 1, 'soil': 1, 'group': 1, 'exp': 1, 'op': 1, 'registration': 1, 'status': 1, 'subsidies': 1, 'finish': 1, 'point': 2, 'received': 1, 'start': 1, 'via': 1, 'a': 1, 'lightings': 1, 'boardings': 1, 'cross': 1, 'street': 3, 'location': 2, 'on': 2, 'routes': 1, 'stop': 2, 'id': 2, 'aircraft': 6, 'make': 4, 'airport': 1, 'icao': 1, 'incident': 1, 'occ': 2, 'ocean': 1, 'operator': 2, 'organization': 5, 'country': 5}\n"
     ]
    }
   ],
   "source": [
    "print(tokenDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "while a<20:\n",
    "    print(attributeList[a].attributeName)\n",
    "    print(attributeList[a].originateFrom)\n",
    "    a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nt小，ITF大，权重大。\n",
    "#token越稀有，权重越大。\n",
    "#N是数据湖中有多少列。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
